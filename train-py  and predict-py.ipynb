{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7583540,"sourceType":"datasetVersion","datasetId":4414520},{"sourceId":10971632,"sourceType":"datasetVersion","datasetId":6826966},{"sourceId":227575747,"sourceType":"kernelVersion"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import argparse\nimport torch\nimport torchvision\nimport numpy as np\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.optim as optim\nimport os\nfrom torchvision import datasets, models\nfrom collections import OrderedDict\nimport sys\n\nsys.argv = [\n    \"train.py\",\n    \"/kaggle/input/oxford-102-flower-dataset/102 flower/flowers\",\n    \"--save_dir\", \"/kaggle/working/\",\n    \"--arch\", \"vgg16\",\n    \"--learning_rate\", \"0.001\",\n    \"--epochs\", \"10\",\n    \"--gpu\"\n]\n\ndef get_input_args():\n    parser = argparse.ArgumentParser(description=\"Train a deep learning model on a dataset\")\n    \n    parser.add_argument('data_dir', type=str, help='Path to dataset directory')\n    parser.add_argument('--save_dir', type=str, default='./', help='Directory to save checkpoint')\n    parser.add_argument('--arch', type=str, default='mobilenet_v2', choices=['mobilenet_v2', 'vgg16', 'vgg13'], help='Model architecture')\n    parser.add_argument('--learning_rate', type=float, default=0.001, help='Learning rate for training')\n    parser.add_argument('--hidden_units', type=int, default=512, help='Number of hidden units in classifier')\n    parser.add_argument('--epochs', type=int, default=10, help='Number of epochs for training')\n    parser.add_argument('--gpu', action='store_true', help='Use GPU for training')\n\n    return parser.parse_args()\n\ndef load_data(data_dir):\n    train_dir = os.path.join(data_dir, 'train')\n    valid_dir = os.path.join(data_dir, 'valid')\n    \n    # Define transforms\n    train_transforms = transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n\n    valid_transforms = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n    \n    # Load datasets\n    train_dataset = datasets.ImageFolder(train_dir, transform=train_transforms)\n    valid_dataset = datasets.ImageFolder(valid_dir, transform=valid_transforms)\n\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=32)\n\n    return train_loader, valid_loader, train_dataset.class_to_idx\n\ndef build_model(arch, hidden_units):\n    if arch == \"mobilenet_v2\":\n        model = models.mobilenet_v2(pretrained=True)\n        input_size = model.classifier[1].in_features\n    elif arch == \"vgg16\":\n        model = models.vgg16(pretrained=True)\n        input_size = model.classifier[0].in_features\n    elif arch == \"vgg13\":\n        model = models.vgg13(pretrained=True)\n        input_size = model.classifier[0].in_features\n    else:\n        raise ValueError(f\"Unsupported architecture: {arch}\")\n\n    # Freeze pretrained parameters\n    for param in model.parameters():\n        param.requires_grad = False\n\n    # Define a new classifier\n    classifier = nn.Sequential(OrderedDict([\n        ('fc1', nn.Linear(input_size, hidden_units)),\n        ('relu', nn.ReLU()),\n        ('dropout', nn.Dropout(0.2)),\n        ('fc2', nn.Linear(hidden_units, 102)),  # 102 flower categories\n        ('output', nn.LogSoftmax(dim=1))\n    ]))\n    if arch == \"mobilenet_v2\":\n        model.fc = classifier\n    else:\n        model.classifier = classifier\n\n    return model\ndef train_model(model, train_loader, valid_loader, device, epochs, learning_rate):\n    criterion = nn.NLLLoss()\n    optimizer = optim.Adam(model.classifier.parameters(), lr=learning_rate)\n\n    model.to(device)\n\n    for epoch in range(epochs):\n        train_loss = 0\n        model.train()\n        \n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n\n            output = model(images)\n            loss = criterion(output, labels)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n\n        # Validation step\n        model.eval()\n        valid_loss = 0\n        accuracy = 0\n\n        with torch.no_grad():\n            for images, labels in valid_loader:\n                images, labels = images.to(device), labels.to(device)\n                output = model(images)\n                loss = criterion(output, labels)\n                valid_loss += loss.item()\n\n                # Calculate accuracy\n                probab = torch.softmax(output, dim=1) \n                top_p, top_class = probab.topk(1, dim=1)\n                equals = top_class == labels.view(*top_class.shape)\n                accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n\n        print(f\"Epoch {epoch+1}/{epochs}.. \"\n              f\"Train Loss: {train_loss/len(train_loader):.3f}.. \"\n              f\"Validation Loss: {valid_loss/len(valid_loader):.3f}.. \"\n              f\"Validation Accuracy: {accuracy/len(valid_loader)*100:.2f}%\")\n\n    return model, optimizer\ndef save_checkpoint(model, save_dir, arch, class_to_idx):\n    checkpoint = {\n        'arch': arch,\n        'state_dict': model.state_dict(),\n        'classifier': model.classifier,\n        'class_to_idx': class_to_idx\n    }\n    save_path = os.path.join(save_dir, 'checkpoint.pth')\n    torch.save(checkpoint, save_path)\n    print(f\"Checkpoint saved at: {save_path}\")\n\ndef main():\n    args = get_input_args()\n    device = torch.device(\"cuda\" if args.gpu and torch.cuda.is_available() else \"cpu\")\n\n    train_loader, valid_loader, class_to_idx = load_data(args.data_dir)\n    model = build_model(args.arch, args.hidden_units)\n\n    model, optimizer = train_model(model, train_loader, valid_loader, device, args.epochs, args.learning_rate)\n    save_checkpoint(model, args.save_dir, args.arch, class_to_idx)\n\nif __name__ == '__main__':\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T12:16:29.778405Z","iopub.execute_input":"2025-03-15T12:16:29.778762Z","iopub.status.idle":"2025-03-15T12:16:51.005610Z","shell.execute_reply.started":"2025-03-15T12:16:29.778738Z","shell.execute_reply":"2025-03-15T12:16:51.004304Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nprint(os.listdir(\"/kaggle/working/\"))\n##torch.save(models.state_dict(), \"/kaggle/working/checkpoint.pth\")\nprint(sys.argv)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T12:39:27.312834Z","iopub.execute_input":"2025-03-15T12:39:27.313115Z","iopub.status.idle":"2025-03-15T12:39:27.318061Z","shell.execute_reply.started":"2025-03-15T12:39:27.313095Z","shell.execute_reply":"2025-03-15T12:39:27.317309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import argparse\nimport torch\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport json\nfrom torchvision import models\nimport os\nimport sys\nsys.argv = [\n    \"predict.py\",\n    \"/kaggle/input/oxford-102-flower-dataset/102 flower/flowers/train/7/image_08103.jpg\",\n    \"/kaggle/working/checkpoint.pth\", \n    \"--name_file\", \"/kaggle/input/cat-to-name-json/cat_to_name.json\", \n    \"--top_k\", \"5\",\n    \"--gpu\"\n]\n\ndef get_input_args():\n    parser = argparse.ArgumentParser(description=\"Predict the class of an image using a trained deep learning model\")\n    \n    parser.add_argument('image_path', type=str, help='Path to input image')\n    parser.add_argument('checkpoint', type=str, help='Path to model checkpoint')\n    parser.add_argument('--name_file', type=str, help='Path to JSON file mapping categories to names')\n    parser.add_argument('--top_k', type=int, default=1, help='Return top K most likely classes')\n    parser.add_argument('--gpu', action='store_true', help='Use GPU for inference')\n\n    return parser.parse_args()\n\ndef load_checkpoint(filepath):\n    \n    checkpoint = torch.load(filepath)\n    model = models.vgg16(pretrained=True)\n    model.name =\"vgg16\"\n\n    for param in model.parameters():\n        param.requires_grad =False\n        \n    model.class_to_idx = checkpoint['class_to_idx']\n    model.classifier = checkpoint['classifier']\n    model.load_state_dict(checkpoint['state_dict'], strict=False)\n    return model\n\ndef process_image(image_path):\n    image = Image.open(image_path)\n    transform = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n    \n    image = Image.open(image_path).convert(\"RGB\") \n    \n    image = transform(image)\n    \n    numpy_image = np.array(image)\n     \n    return torch.unsqueeze(image, 0)\ndef predict(image, model, cat_to_name=None, topk=5, device=\"cpu\"):\n    model.eval()\n    image = image.to(device)\n    \n    with torch.no_grad():\n        output = model.forward(image)\n    \n    probab = torch.exp(output)\n    probs, indices = probab.topk(topk, dim=1)\n    \n    list_probs = probs.tolist()[0]\n    list_indices = indices.tolist()[0]\n    \n    index_map = {v: k for k, v in model.class_to_idx.items()}\n    classes = [index_map[idx] for idx in list_indices]\n\n    # Convert class indices to actual names if `cat_to_name` is provided\n    if cat_to_name:\n        labels = [cat_to_name[c] for c in classes]\n        return list_probs, labels\n    \n    return list_probs, classes\n    \ndef print_predictions(probabilities, classes, cat_to_name=None):\n    labels = [cat_to_name.get(c, f\"Class {c}\") for c in classes] if cat_to_name else classes\n\n    for i, (probab, label, c) in enumerate(zip(probabilities, labels, classes), 1):\n        print(f\"{i}) {probab*100:.2f}% - {label.title()} (Class {c})\")\n\ndef main():\n    args = get_input_args()\n\n    with open(args.name_file, 'r') as f:\n                cat_to_name = json.load(f)\n\n    model = load_checkpoint(args.checkpoint)\n    \n    image = process_image(args.image_path)\n    \n    device = torch.device(\"cuda\" if args.gpu and torch.cuda.is_available() else \"cpu\")\n    \n    model.to(device)\n\n    probs, classes = predict(image, model, cat_to_name, args.top_k, device)\n    print_predictions(probs, classes)\n    \n      \nif __name__ == '__main__':\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T12:43:44.782541Z","iopub.execute_input":"2025-03-15T12:43:44.782879Z","iopub.status.idle":"2025-03-15T12:43:46.535948Z","shell.execute_reply.started":"2025-03-15T12:43:44.782854Z","shell.execute_reply":"2025-03-15T12:43:46.535199Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nimage_path = \"/kaggle/input/oxford-102-flower-dataset/102 flower/flowers/train/7/image_08103.jpg\"\n\n# Load and display the image\nimg = mpimg.imread(image_path)\nplt.imshow(img)\nplt.axis(\"off\")  # Hide axis\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T12:45:49.417788Z","iopub.execute_input":"2025-03-15T12:45:49.418099Z","iopub.status.idle":"2025-03-15T12:45:49.740708Z","shell.execute_reply.started":"2025-03-15T12:45:49.418072Z","shell.execute_reply":"2025-03-15T12:45:49.739775Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\ndataset_path = \"/kaggle/input/oxford-102-flower-dataset/102 flower/flowers/train\"\nclass_folders = os.listdir(dataset_path)\n\nfor class_folder in class_folders[:5]:  # Checking first 5 class folders\n    class_path = os.path.join(dataset_path, class_folder)\n    if os.path.isdir(class_path):\n        print(f\"Checking {class_path}:\")\n        print(os.listdir(class_path)[:10])  # Print first 10 files inside\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}